{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from seqeval.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from utils_seq_labeling import (\n",
    "    convert_examples_to_features,\n",
    "    get_labels,\n",
    "    read_examples_from_file,\n",
    ")\n",
    "from modeling_layoutlm import LayoutLMForTokenClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from transformers import RobertaConfig, RobertaForTokenClassification, RobertaTokenizer\n",
    "from transformers import (\n",
    "    DistilBertConfig,\n",
    "    DistilBertForTokenClassification,\n",
    "    DistilBertTokenizer,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = sum(\n",
    "    (\n",
    "        tuple(conf.pretrained_config_archive_map.keys())\n",
    "        for conf in (BertConfig, RobertaConfig, DistilBertConfig)\n",
    "    ),\n",
    "    (),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_layoutlm_2 import LayoutLMForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n",
    "    \"distilbert\": (\n",
    "        DistilBertConfig,\n",
    "        DistilBertForTokenClassification,\n",
    "        DistilBertTokenizer,\n",
    "    ),\n",
    "    \"layoutlm\": (BertConfig, LayoutLMForTokenClassification, BertTokenizer),\n",
    "}\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_seq_labeling import load_and_cache_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "## Required parameters\n",
    "parser.add_argument(\n",
    "    \"--data_dir\",\n",
    "    default='data_xt',\n",
    "    type=str,\n",
    "    required=False,\n",
    "    help=\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_type\",\n",
    "    default='layoutlm',\n",
    "    type=str,\n",
    "    required=False,\n",
    "    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default='pretrained/layoutlm-large-uncased',\n",
    "    #default='pretrained/layoutlm-base-uncased',\n",
    "    # default='pretrained/model',\n",
    "    type=str,\n",
    "    required=False,\n",
    "    help=\"Path to pre-trained model or shortcut name selected in the list: \"\n",
    "    + \", \".join(ALL_MODELS),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default='pretrained/model',\n",
    "    type=str,\n",
    "    required=False,\n",
    "    help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    ")\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\n",
    "    \"--labels\",\n",
    "    default=\"data_xt/labels.txt\",\n",
    "    type=str,\n",
    "    help=\"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_name\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Pretrained config name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_length\",\n",
    "    default=512,\n",
    "    type=int,\n",
    "    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "    \"than this will be truncated, sequences shorter will be padded.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\",\n",
    "    action=\"store_true\",\n",
    "    default=True,\n",
    "    help=\"Set this flag if you are using an uncased model.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_eval_batch_size\",\n",
    "    default=2,\n",
    "    type=int,\n",
    "    help=\"Batch size per GPU/CPU for evaluation.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_cuda\", action=\"store_true\",\n",
    "    help=\"Avoid using CUDA when available\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_output_dir\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Overwrite the content of the output directory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Overwrite the cached training and evaluation sets\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=42, help=\"random seed for initialization\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    default=True,\n",
    "    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16_opt_level\",\n",
    "    type=str,\n",
    "    default=\"O1\",\n",
    "    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "    \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"For distributed training: local_rank\",\n",
    ")\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(args, 'per_gpu_train_batch_size', 12 * 2)\n",
    "setattr(args, 'per_gpu_train_batch_size', 2 * 1)\n",
    "setattr(args, 'max_steps', -1)\n",
    "setattr(args, 'gradient_accumulation_steps', 1)\n",
    "setattr(args, 'num_train_epochs', 100.0)\n",
    "setattr(args, 'weight_decay', 0.0)\n",
    "setattr(args, 'learning_rate', 5e-5)\n",
    "setattr(args, 'adam_epsilon', 1e-8)\n",
    "setattr(args, 'warmup_steps', 0)\n",
    "setattr(args, 'max_grad_norm', 1.0)\n",
    "setattr(args, 'logging_steps', 50)\n",
    "setattr(args, 'save_steps', 50)\n",
    "setattr(args, 'output_dir', 'output')\n",
    "setattr(args, 'max_seq_length', 512)\n",
    "setattr(args, 'local_rank', -1)\n",
    "setattr(args, 'n_gpu', 1)\n",
    "setattr(args, 'no_cuda', True)\n",
    "setattr(args, 'fp16', False)\n",
    "setattr(args, 'do_lower_case', True)\n",
    "setattr(args, 'labels', 'data_xt/labels.txt')\n",
    "setattr(args, 'data_dir', 'data_xt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\n",
    "        \"cuda:0\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "    )\n",
    "    if args.no_cuda:\n",
    "        args.n_gpu = 0\n",
    "    else:\n",
    "        torch.cuda.set_device(device)\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "        \n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(args.output_dir, \"train.log\"),\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Prepare CONLL-2003 task\n",
    "labels = get_labels(args.labels)\n",
    "num_labels = len(labels)\n",
    "# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "model.to(args.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_and_cache_examples(\n",
    "    args, tokenizer, labels, pad_token_label_id, mode=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- show some batch elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(168) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataset):\n",
    "    if args.model_type != \"layoutlm\":\n",
    "        batch = batch[:4]\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": batch[0],\n",
    "        \"attention_mask\": batch[1],\n",
    "        \"labels\": batch[3],\n",
    "    }\n",
    "    \n",
    "    if args.model_type == \"layoutlm\":\n",
    "        inputs[\"bbox\"] = batch[4]\n",
    "        \n",
    "    if args.model_type != \"distilbert\":\n",
    "        inputs[\"token_type_ids\"] = (\n",
    "            batch[2]\n",
    "            if args.model_type in [\"bert\", \"xlnet\", \"layoutlm\"]\n",
    "            else None\n",
    "        )  # XLM and RoBERTa don\"t use segment_ids\n",
    "        \n",
    "    bbox = inputs['bbox']\n",
    "    print((bbox[:, 2] - bbox[:, 0]).max(), (bbox[:, 2] - bbox[:, 0]).min())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"train\"\n",
    "examples = read_examples_from_file(args.data_dir, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['HARDOUIN', 'Edwige', 'PERMIS'],\n",
       " ['B-IDENTITY_GROUP', 'E-IDENTITY_GROUP', 'B-IDENTITY_GROUP'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0].words[:3], examples[0].labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "if args.local_rank in [-1, 0]:\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "train_sampler = (\n",
    "    RandomSampler(train_dataset)\n",
    "    if args.local_rank == -1\n",
    "    else DistributedSampler(train_dataset)\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=train_sampler, batch_size=args.train_batch_size\n",
    ")\n",
    "\n",
    "if args.max_steps > 0:\n",
    "    t_total = args.max_steps\n",
    "    args.num_train_epochs = (\n",
    "        args.max_steps\n",
    "        // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "        + 1\n",
    "    )\n",
    "else:\n",
    "    t_total = (\n",
    "        len(train_dataloader)\n",
    "        // args.gradient_accumulation_steps\n",
    "        * args.num_train_epochs\n",
    "    )\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "if args.fp16 and not args.no_cuda:\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "        )\n",
    "    model, optimizer = amp.initialize(\n",
    "        model, optimizer, opt_level=args.fp16_opt_level\n",
    "    )\n",
    "\n",
    "# multi-gpu training (should be after apex fp16 initialization)\n",
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Distributed training (should be after apex fp16 initialization)\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model,\n",
    "        device_ids=[args.local_rank],\n",
    "        output_device=args.local_rank,\n",
    "        find_unused_parameters=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "logger.info(\n",
    "    \"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size\n",
    ")\n",
    "logger.info(\n",
    "    \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "    args.train_batch_size\n",
    "    * args.gradient_accumulation_steps\n",
    "    * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    ")\n",
    "logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### step by step train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args)  # Added here for reproductibility (even between python 2 and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 0/201 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "epoch_iterator = enumerate(tqdm(\n",
    "    train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:   0%|          | 1/201 [00:00<02:04,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Iteration:  31%|███▏      | 63/201 [00:00<01:00,  2.29it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 5\n"
     ]
    }
   ],
   "source": [
    "for i in range(70):\n",
    "    step, batch = next(epoch_iterator)\n",
    "print (step, len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iteration:  36%|███▋      | 73/201 [06:32<2:39:58, 74.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(662), tensor(0))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step, batch = next(epoch_iterator)\n",
    "print (step, len(batch))\n",
    "\n",
    "if args.model_type != \"layoutlm\":\n",
    "    batch = batch[:4]\n",
    "batch = tuple(t.to(args.device) for t in batch)\n",
    "inputs = {\n",
    "    \"input_ids\": batch[0],\n",
    "    \"attention_mask\": batch[1],\n",
    "    \"labels\": batch[3],\n",
    "}\n",
    "if args.model_type == \"layoutlm\":\n",
    "    inputs[\"bbox\"] = batch[4]\n",
    "if args.model_type != \"distilbert\":\n",
    "    inputs[\"token_type_ids\"] = (\n",
    "        batch[2]\n",
    "        if args.model_type in [\"bert\", \"xlnet\", \"layoutlm\"]\n",
    "        else None\n",
    "    )  # XLM and RoBERTa don\"t use segment_ids\n",
    "    \n",
    "bbox = inputs['bbox']\n",
    "(bbox[:, :, 2] - bbox[:, :, 0]).max(), (bbox[:, :, 2] - bbox[:, :, 0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "if args.model_type != \"layoutlm\":\n",
    "    batch = batch[:4]\n",
    "batch = tuple(t.to(args.device) for t in batch)\n",
    "inputs = {\n",
    "    \"input_ids\": batch[0],\n",
    "    \"attention_mask\": batch[1],\n",
    "    \"labels\": batch[3],\n",
    "}\n",
    "if args.model_type == \"layoutlm\":\n",
    "    inputs[\"bbox\"] = batch[4]\n",
    "if args.model_type != \"distilbert\":\n",
    "    inputs[\"token_type_ids\"] = (\n",
    "        batch[2]\n",
    "        if args.model_type in [\"bert\", \"xlnet\", \"layoutlm\"]\n",
    "        else None\n",
    "    )  # XLM and RoBERTa don\"t use segment_ids\n",
    "\n",
    "outputs = model(**inputs)\n",
    "loss = outputs[\n",
    "    0\n",
    "]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "if args.n_gpu > 1:\n",
    "    loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "if args.gradient_accumulation_steps > 1:\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "if args.fp16:\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "else:\n",
    "    loss.backward()\n",
    "\n",
    "tr_loss += loss.item()\n",
    "if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "    if args.fp16:\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            amp.master_params(optimizer), args.max_grad_norm\n",
    "        )\n",
    "    else:\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), args.max_grad_norm\n",
    "        )\n",
    "\n",
    "    scheduler.step()  # Update learning rate schedule\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "    global_step += 1\n",
    "\n",
    "print (tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = trange(\n",
    "    int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    ")\n",
    "\n",
    "set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
    "\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        model.train()\n",
    "        if args.model_type != \"layoutlm\":\n",
    "            batch = batch[:4]\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"labels\": batch[3],\n",
    "        }\n",
    "        if args.model_type == \"layoutlm\":\n",
    "            inputs[\"bbox\"] = batch[4]\n",
    "        if args.model_type != \"distilbert\":\n",
    "            inputs[\"token_type_ids\"] = (\n",
    "                batch[2]\n",
    "                if args.model_type in [\"bert\", \"xlnet\", \"layoutlm\"]\n",
    "                else None\n",
    "            )  # XLM and RoBERTa don\"t use segment_ids\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[\n",
    "            0\n",
    "        ]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "        if args.fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            if args.fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    amp.master_params(optimizer), args.max_grad_norm\n",
    "                )\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), args.max_grad_norm\n",
    "                )\n",
    "                \n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "    if args.max_steps > 0 and global_step > args.max_steps:\n",
    "        train_iterator.close()\n",
    "        break\n",
    "\n",
    "if args.local_rank in [-1, 0]:\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
